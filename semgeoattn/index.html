<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>Learning Human Visual Attention on 3D Surfaces through Geometry-Queried Semantic Priors</title>
    <link rel="icon" type="image/x-icon" href="assets/images/favicon.ico">
    <meta content="Learning Human Visual Attention on 3D Surfaces through Geometry-Queried Semantic Priors" name="description" />
    <meta content="summary" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="static/css/template.css" rel="stylesheet" type="text/css" />
    <link href="static/css/style.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Roboto:300,regular,500"]
            }
        });
    </script>
    <script type="text/javascript">
        ! function (o, c) {
            var n = c.documentElement,
                t = " w-mod-";
            n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
        }(window, document);
    </script>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
</head>

<body>
    <!-- Hero Section -->
    <div class="section hero nerf-_v2">
        <div class="container-2 nerf_header_v2 w-container">
            <h1 class="nerf_title_v2">Learning Human Visual Attention on 3D Surfaces <br> through Geometry-Queried Semantic Priors</h1>
            <div class="nerf_subheader_v2">Conference Name 2025</div>
            <div class="nerf_subheader_v2">
                <div>
                    <!-- Authors - Update these with actual information -->
                    <a href="sohampahari.github.io" target="_blank" class="nerf_authors_v2">Soham Pahari<span class="text-span_nerf"></span></a><sup> 1</sup>,&nbsp;&nbsp;
                    <a href="#" target="_blank" class="nerf_authors_v2">Sandeep Chand Kumain<span class="text-span_nerf"></span></a><sup> 1</sup>,&nbsp;&nbsp;
                    <!-- <a href="#" target="_blank" class="nerf_authors_v2">Author Three<span class="text-span_nerf"></span></a><sup> 2</sup>,&nbsp;&nbsp;
                    <a href="#" target="_blank" class="nerf_authors_v2">Author Four<span class="text-span_nerf"></span></a><sup> 1,2</sup> -->
                    <!-- Add more authors as needed -->
                </div>
                <div>
                    <!-- Affiliations - Update these -->
                    <h1 class="nerf_affiliation_v2"><sup>1 </sup>University of Petroleum and Energy Studies</h1>,
                    <!-- <h1 class="nerf_affiliation_v2"><sup>2 </sup>University Two</h1> -->
                    <!-- Add more affiliations as needed -->
                </div>

                <div class="external-link">
                    <a class="btn" href="https://arxiv.org/abs/2602.06419" role="button" target="_blank">
                        <i class="ai ai-arxiv"></i> ArXiv </a>
                    <a class="btn" href="#" role="button" target="_blank">
                        <i class="fa fa-file-pdf"></i> Paper </a>
                    <a class="btn" href="https://github.com/sohampahari/SemGeoAttnNet" role="button" target="_blank">
                        <i class="fa-brands fa-github"></i> Code </a>
                </div>

            </div>
        </div>
    </div>

    <!-- Abstract Section -->
    <div data-anchor="slide1" class="section nerf_section">
        <div class="w-container grey_container">
            <h2 class="grey-heading_nerf">Abstract</h2>
            <p class="paragraph-3 nerf_text nerf_results_text">
                Human visual attention on three-dimensional objects emerges from the interplay between bottom-up geometric processing and top-down semantic recognition. Existing 3D saliency methods rely on hand-crafted geometric features or learning-based approaches that lack semantic awareness, failing to explain why humans fixate on semantically meaningful but geometrically unremarkable regions. We introduce <b style="color: rgb(65, 150, 200);">SemGeo-AttentionNet</b>, a dual-stream architecture that explicitly formalizes this dichotomy through asymmetric cross-modal fusion, leveraging diffusion-based semantic priors from geometry-conditioned multi-view rendering and point cloud transformers for geometric processing. Cross-attention ensures geometric features query semantic content, enabling bottom-up distinctiveness to guide top-down retrieval. We extend our framework to temporal scanpath generation through reinforcement learning, introducing the first formulation respecting 3D mesh topology with inhibition-of-return dynamics. Evaluation on SAL3D, NUS3D and 3DVA datasets demonstrates substantial improvements, validating how cognitively motivated architectures effectively model human visual attention on three-dimensional surfaces.
            </p>
        </div>
    </div>

    <!-- Pipeline Section -->
    <div class="white_section_nerf w-container">
        <h2 class="grey-heading_nerf">Method Overview</h2>
        <div class="grid-container-1">
            <img src="assets/images/pipe_line.png" alt="SemGeo-AttentionNet Pipeline" style="max-width: 100%;">
            <p class="paragraph-3 nerf_text" style="margin-top: 20px;">
                Our dual-stream architecture combines bottom-up geometric processing via point cloud transformers with top-down semantic recognition through diffusion-based priors. The cross-attention mechanism allows geometric features to query semantic content, enabling precise prediction of human visual attention on 3D surfaces.
            </p>
        </div>
    </div>

    <!-- ========== QUALITATIVE RESULTS SECTION ========== -->
    <div class="section nerf_section">
        <div class="w-container grey_container">
            <h2 class="grey-heading_nerf">Qualitative Results</h2>

            <!-- 3D Saliency Video Visualization -->
            <div class="results-section">
                <h3 class="sub_heading_results">3D Saliency Visualization on SAL3D</h3>
                <p class="paragraph-3 nerf_text" style="text-align: center; margin-bottom: 20px;">
                    Comparison of Ground Truth saliency maps with our predictions
                </p>

                <!-- Video Grid: 4 columns - GT | Pred | GT | Pred -->
                <div class="video-grid-4">
                    <div class="video-item">
                        <p class="video-label">Ground Truth</p>
                        <video class="result-video" loop playsinline autoplay muted src="assets/videos/cat_gt.mp4"></video>
                        <p class="video-caption">Cat</p>
                    </div>
                    <div class="video-item">
                        <p class="video-label">Ours</p>
                        <video class="result-video" loop playsinline autoplay muted src="assets/videos/cat_pred.mp4"></video>
                        <p class="video-caption">Cat</p>
                    </div>
                    <div class="video-item">
                        <p class="video-label">Ground Truth</p>
                        <video class="result-video" loop playsinline autoplay muted src="assets/videos/gorilla_gt.mp4"></video>
                        <p class="video-caption">Gorilla</p>
                    </div>
                    <div class="video-item">
                        <p class="video-label">Ours</p>
                        <video class="result-video" loop playsinline autoplay muted src="assets/videos/gorilla_pred.mp4"></video>
                        <p class="video-caption">Gorilla</p>
                    </div>
                </div>

                <!-- Second row -->
                <div class="video-grid-4" style="margin-top: 20px;">
                    <div class="video-item">
                        <p class="video-label">Ground Truth</p>
                        <video class="result-video" loop playsinline autoplay muted src="assets/videos/horse_gt.mp4"></video>
                        <p class="video-caption">Horse</p>
                    </div>
                    <div class="video-item">
                        <p class="video-label">Ours</p>
                        <video class="result-video" loop playsinline autoplay muted src="assets/videos/horse_pred.mp4"></video>
                        <p class="video-caption">Horse</p>
                    </div>
                    <div class="video-item">
                        <p class="video-label">Ground Truth</p>
                        <video class="result-video" loop playsinline autoplay muted src="assets/videos/octopus_gt.mp4"></video>
                        <p class="video-caption">Octopus</p>
                    </div>
                    <div class="video-item">
                        <p class="video-label">Ours</p>
                        <video class="result-video" loop playsinline autoplay muted src="assets/videos/octopus_pred.mp4"></video>
                        <p class="video-caption">Octopus</p>
                    </div>
                </div>
            </div>

            <!-- Static Comparison Images Subsection -->
            <div class="results-section" style="margin-top: 40px;">
                <h3 class="sub_heading_results">Visual Comparisons on NUS3D Dataset</h3>
                <p class="paragraph-3 nerf_text" style="text-align: center;">
                    Comparison of saliency predictions across different methods
                </p>
                <div class="comparison-gallery">
                    <div class="comparison-item">
                        <img src="assets/comparisons/comparison_012.png" alt="Comparison 1">
                    </div>
                    <div class="comparison-item">
                        <img src="assets/comparisons/comparison_111.png" alt="Comparison 2">
                    </div>
                    <div class="comparison-item">
                        <img src="assets/comparisons/comparison_277.png" alt="Comparison 3">
                    </div>
                    <div class="comparison-item">
                        <img src="assets/comparisons/comparison_368.png" alt="Comparison 4">
                    </div>
                    <div class="comparison-item">
                        <img src="assets/comparisons/comparison_401.png" alt="Comparison 5">
                    </div>
                    <div class="comparison-item">
                        <img src="assets/comparisons/comparison_424.png" alt="Comparison 6">
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- ========== QUANTITATIVE RESULTS SECTION ========== -->
    <div class="white_section_nerf w-container">
        <h2 class="grey-heading_nerf">Quantitative Results</h2>

        <!-- SAL3D Results -->
        <div class="results-section">
            <h3 class="sub_heading_results">SAL3D Dataset</h3>
            <p class="paragraph-3 nerf_text">28% improvement in CC over previous state-of-the-art</p>
            <table class="results-table">
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>CC ↑</th>
                        <th>KL-Div ↓</th>
                        <th>MSE ↓</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>Song et al.</td><td>0.1249</td><td>0.7034</td><td>0.3220</td></tr>
                    <tr><td>Nousias et al.</td><td>0.0570</td><td>1.9618</td><td>0.0759</td></tr>
                    <tr><td>SAL3D model</td><td>0.6616</td><td>0.3051</td><td>0.0204</td></tr>
                    <tr><td>Mesh Mamba</td><td>0.6140</td><td>0.3067</td><td>-</td></tr>
                    <tr class="highlight"><td><b>Ours (SemGeo-Attn)</b></td><td><b>0.8492</b></td><td><b>0.1638</b></td><td><b>0.0114</b></td></tr>
                </tbody>
            </table>
        </div>

        <!-- NUS3D Results -->
        <div class="results-section">
            <h3 class="sub_heading_results">NUS3D-Saliency Dataset</h3>
            <p class="paragraph-3 nerf_text">129% improvement in LCC over MIMO-GAN</p>
            <table class="results-table">
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>LCC ↑</th>
                        <th>AUC ↑</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>DSM</td><td>0.222</td><td>0.726</td></tr>
                    <tr><td>MIMO-GAN-A1</td><td>0.290</td><td>0.781</td></tr>
                    <tr><td>MIMO-GAN-A2</td><td>0.057</td><td>0.584</td></tr>
                    <tr><td>MIMO-GAN-A3</td><td>0.259</td><td>0.753</td></tr>
                    <tr><td>MIMO-GAN</td><td>0.267</td><td>0.761</td></tr>
                    <tr class="highlight"><td><b>Ours (SemGeo-Attn)</b></td><td><b>0.609</b></td><td><b>0.935</b></td></tr>
                </tbody>
            </table>
        </div>

        <!-- 3DVA Results -->
        <div class="results-section">
            <h3 class="sub_heading_results">3DVA Dataset</h3>
            <p class="paragraph-3 nerf_text">49% improvement in Mean LCC over MIMO-GAN-CRF</p>
            <table class="results-table">
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Mean LCC ↑</th>
                        <th>Std. Dev. ↓</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>Multi-Scale Gaussian</td><td>0.131</td><td>0.265</td></tr>
                    <tr><td>Diffusion Wavelets</td><td>0.088</td><td>0.222</td></tr>
                    <tr><td>Spectral Processing</td><td>0.078</td><td>0.253</td></tr>
                    <tr><td>Point Clustering</td><td>0.132</td><td>0.300</td></tr>
                    <tr><td>Salient Regions</td><td>0.215</td><td>0.245</td></tr>
                    <tr><td>Hilbert-CNN</td><td>0.113</td><td>0.267</td></tr>
                    <tr><td>RPCA</td><td>0.199</td><td>0.251</td></tr>
                    <tr><td>CfS-CNN</td><td>0.226</td><td>0.243</td></tr>
                    <tr><td>MIMO-GAN-CRF</td><td>0.510</td><td>0.108</td></tr>
                    <tr class="highlight"><td><b>Ours (SemGeo-Attn)</b></td><td><b>0.762</b></td><td><b>0.093</b></td></tr>
                </tbody>
            </table>
        </div>
    </div>

    <!-- Scanpath Results -->
    <div class="section nerf_section">
        <div class="w-container grey_container">
            <h2 class="grey-heading_nerf">Scanpath Generation</h2>
            <div class="scanpath-results">
                <p class="paragraph-3 nerf_text">
                    Our RL-based scanpath generator achieves <b>NSS of 2.05</b> and <b>MultiMatch score of 0.51</b> on NUS3D, indicating that temporal fixation sequences closely match human viewing behavior. The policy successfully balances saliency-driven attention with spatial exploration and inhibition of return.
                </p>
                <div class="metric-cards">
                    <div class="metric-card">
                        <div class="metric-value">2.05</div>
                        <div class="metric-label">NSS (SAL3D)</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">1.60</div>
                        <div class="metric-label">NSS (NUS3D)</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">0.51</div>
                        <div class="metric-label">MultiMatch</div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- BibTeX Section -->
    <div class="white_section_nerf grey_container w-container">
        <h2 class="grey-heading_nerf">BibTeX</h2>
        <div class="bibtex">
            <pre><code>@article{semgeoattn2025,
    author    = {Author One and Author Two and Author Three and Author Four},
    title     = {Learning Human Visual Attention on 3D Surfaces through Geometry-Queried Semantic Priors},
    journal   = {arxiv:XXXX.XXXXX},
    year      = {2025},
}</code></pre>
        </div>
    </div>

</body>

<footer>
    This project page template is inspired by <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. All rights reserved.
</footer>

</html>
