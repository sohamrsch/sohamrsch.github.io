<!DOCTYPE html>
<head>
    <meta charset="utf-8" />
    <title>TriadNet: Graph-Enhanced Multimodal Learning for Personality Trait Assessment</title>
    <meta content="TriadNet: Graph-Enhanced Multimodal Learning for Personality Trait Assessment" name="description" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="static/css/template.css" rel="stylesheet" type="text/css" />
    <link href="static/css/my_style.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Ubuntu:300,400,500,700|Roboto:300,400,500|Open+Sans:300,400,600" rel="stylesheet">
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Lato:100,300,400,700","Montserrat:300,400,500,600,700","Ubuntu:300,400,500,700","Open Sans:300,400,600,700","Roboto:300,400,500"]
            }
        });
    </script>
    <script type="text/javascript" src="static/js/zoom.js"></script>
</head>

<body>

<!-- ===================== HERO ===================== -->
<div class="section hero nerf-_v2">
    <div class="container-2 nerf_header_v2 w-container">

        <h1 class="nerf_title_v2">
            TriadNet: <span class="hero-accent">Graph-Enhanced Multimodal Learning</span><br>
            for Personality Trait Assessment
        </h1>

        <div class="nerf_subheader_v2" style="color:#888; font-size:1rem; margin-top:6px; margin-bottom:0;">
            IEEE ‚Äî 2025
        </div>

        <!-- Authors -->
        <div class="nerf_subheader_v2" style="margin-top:18px;">
            <div style="font-size:1.05rem;">
                <a href="https://sohampahari.github.io" target="_blank" class="nerf_authors_v2">Soham Pahari</a><sup>*1</sup>,&nbsp;&nbsp;
                <a href="https://scholar.google.com/citations?user=3GWFyU4AAAAJ&hl=en" class="nerf_authors_v2">Sandeep Chand Kumain</a><sup>‚Ä†1</sup>,&nbsp;&nbsp;
                <a href="https://scholar.google.com/citations?user=cv7Xbu8AAAAJ&hl=en" class="nerf_authors_v2">Lalit K. Awasthi</a><sup>2</sup>,&nbsp;&nbsp;
                <a href="https://scholar.google.com/citations?user=zSULiQ4AAAAJ&hl=en" class="nerf_authors_v2">Maheep Singh</a><sup>3</sup>
            </div>
            <div style="margin-top:10px; font-size:0.92rem; color:#666;">
                <sup>1</sup> School of Computer Science, UPES, Dehradun, India &nbsp;|&nbsp;
                <sup>2</sup> SPU Mandi, Himachal Pradesh, India &nbsp;|&nbsp;
                <sup>3</sup> Doon University, Dehradun, India
            </div>
            <div style="margin-top:4px; font-size:0.8rem; color:#999;">
                <sup>*</sup>Lead author &nbsp; <sup>‚Ä†</sup>Corresponding author
            </div>
        </div>

        <!-- Buttons -->
        <div class="external-link" style="margin-top:24px;">
            <a class="btn" href="https://github.com/sohampahari/TriadNet" role="button" target="_blank">
                <i class="fa-brands fa-github"></i>&nbsp; Code
            </a>
            <a class="btn" href="#" role="button" target="_blank">
                <i class="ai ai-arxiv"></i>&nbsp; arXiv
            </a>
            <a class="btn" href="#" role="button" target="_blank">
                <i class="fa fa-file-pdf"></i>&nbsp; Paper
            </a>
            <a class="btn" href="https://chalearnlap.cvc.uab.cat/" role="button" target="_blank">
                <i class="fa fa-database"></i>&nbsp; Dataset
            </a>
        </div>

    </div>
</div>

<!-- ===================== STATS BAR ===================== -->
<div class="section nerf_section" style="padding:10px 0 20px;">
    <div class="stats-bar">
        <div class="stat-item">
            <div class="stat-value"><span>97.8%</span></div>
            <div class="stat-label">Cluster Accuracy</div>
        </div>
        <div class="stat-item">
            <div class="stat-value"><span>96.9%</span></div>
            <div class="stat-label">Mean Trait Accuracy</div>
        </div>
        <div class="stat-item">
            <div class="stat-value"><span>0.0116</span></div>
            <div class="stat-label">Mean Squared Error</div>
        </div>
        <div class="stat-item">
            <div class="stat-value"><span>~10K</span></div>
            <div class="stat-label">Training Videos</div>
        </div>
        <div class="stat-item">
            <div class="stat-value"><span>65M</span></div>
            <div class="stat-label">Parameters</div>
        </div>
    </div>
</div>

<!-- ===================== ABSTRACT ===================== -->
<div data-anchor="abstract" class="section nerf_section">
    <div class="w-container grey_container">
        <h2 class="grey-heading_nerf">Abstract</h2>
        <p class="paragraph-3 nerf_text nerf_results_text">
            Personality trait assessment (PTA) remains a central challenge in affective computing, as single-modality
            systems often fail to represent the diverse behavioral cues underlying human personality.
            This work presents <strong>TriadNet</strong>, a multimodal architecture that jointly models visual, auditory,
            and textual information for Big Five trait prediction.
            The visual stream employs a <strong>Dynamic Face Graph Network (DFGN)</strong> to capture
            spatial‚Äìtemporal facial variations, the audio stream uses a <strong>Prosody-Aware Capsule Network (PACN)</strong>
            to encode prosodic and semantic patterns, and the textual stream incorporates a
            <strong>Contextual Sentiment Flow (CSF)</strong> module to track emotional and linguistic shifts.
            These representations are unified through <strong>Iterative Modality Dialogue (IMD)</strong> with cross-attention,
            followed by a <strong>Trait-Specific Attention Gate (TSAG)</strong> for individualized reasoning.
            Evaluations on the ChaLearn LAP 2017 dataset show notable gains, achieving
            <strong>97.8% cluster accuracy</strong> and <strong>96.9% mean trait accuracy</strong>,
            establishing TriadNet as a scalable framework for automated and interpretable PTA in real-world settings.
        </p>
    </div>
</div>

<!-- ===================== ARCHITECTURE OVERVIEW ===================== -->
<div class="white_section_nerf w-container">
    <h2 class="grey-heading_nerf">Architecture Overview</h2>

    <!-- Modality pills -->
    <div class="modality-row">
        <div class="modality-pill pill-video">
            <div class="pill-icon">üé•</div> Visual Stream (DFGN)
        </div>
        <div class="modality-pill pill-audio">
            <div class="pill-icon">üéôÔ∏è</div> Audio Stream (PACN)
        </div>
        <div class="modality-pill pill-text">
            <div class="pill-icon">üìù</div> Text Stream (CSF)
        </div>
    </div>

    <!-- Architecture image -->
    <img class="section-img" src="static/images/ocean.png"
         alt="TriadNet Architecture Overview"
         onerror="this.style.display='none'; this.nextElementSibling.style.display='block'">
    <div style="display:none; background:#f2f2f2; border-radius:8px; padding:60px 20px; text-align:center; margin:20px 25px 6px; color:#aaa; font-family:Roboto,sans-serif;">
        [ Architecture diagram ‚Äî place <code>ocean.png</code> in <code>static/images/</code> ]
    </div>
    <p class="fig-caption">
        Fig. 1. Proposed multimodal architecture for Big Five personality trait prediction.
        Three dedicated encoders independently process video, audio, and text inputs; their 512-D embeddings
        are fused via IMD and refined by TSAG before hierarchical prediction.
    </p>

    <!-- Pipeline flow -->
    <div class="pipeline-grid">
        <div class="pipeline-step">
            <div class="step-label">Video</div>
            <div class="step-name">DFGN<br><small style="color:#999">512-D</small></div>
        </div>
        <div class="pipeline-arrow">‚Üí</div>
        <div class="pipeline-step">
            <div class="step-label">Fusion</div>
            <div class="step-name">IMD<br><small style="color:#999">1536-D</small></div>
        </div>
        <div class="pipeline-arrow">‚Üí</div>
        <div class="pipeline-step">
            <div class="step-label">Attention</div>
            <div class="step-name">TSAG<br><small style="color:#999">per-trait</small></div>
        </div>
        <div class="pipeline-arrow">‚Üí</div>
        <div class="pipeline-step">
            <div class="step-label">Output</div>
            <div class="step-name">Big Five<br><small style="color:#999">5 scores</small></div>
        </div>
    </div>
    <div style="text-align:center; font-size:0.8rem; color:#bbb; font-family:Roboto,sans-serif; margin:-8px 0 18px;">
        Audio (PACN, 512-D) and Text (CSF, 512-D) also feed into IMD
    </div>

</div>

<!-- ===================== COMPONENTS ===================== -->
<div class="section nerf_section">
    <div class="w-container grey_container" style="padding-bottom:30px;">
        <h2 class="grey-heading_nerf">Key Components</h2>
        <div class="component-grid">

            <div class="component-card">
                <div class="card-tag">Video ¬∑ DFGN</div>
                <h3>Dynamic Face Graph Network</h3>
                <p>
                    Models 68 facial landmarks as a weighted graph using Euclidean distances.
                    Graph convolution layers (256‚Üí128‚Üí64-D) capture spatial structure, while
                    an LSTM sequences frame-level embeddings for temporal dynamics.
                    Fused with Emotion-Net appearance features via a learned gate.
                    Output: 512-D video embedding.
                </p>
            </div>

            <div class="component-card">
                <div class="card-tag">Audio ¬∑ PACN</div>
                <h3>Prosody-Aware Capsule Network</h3>
                <p>
                    Combines low-level MFCCs (39-D) with high-level Wav2Vec2 embeddings (768-D)
                    into a dual-stream descriptor. A 1-D convolution extracts local temporal
                    patterns, which are then routed through capsule layers via dynamic
                    routing-by-agreement. Output: 512-D audio embedding.
                </p>
            </div>

            <div class="component-card">
                <div class="card-tag">Text ¬∑ CSF</div>
                <h3>Contextual Sentiment Flow</h3>
                <p>
                    Encodes utterances with BERT (768-D contextual embeddings), then sequences
                    them through an LSTM to model sentiment evolution across turns.
                    A soft attention mechanism highlights personality-relevant phrases.
                    Output: 512-D text embedding via a 2-layer MLP.
                </p>
            </div>

            <div class="component-card">
                <div class="card-tag">Fusion ¬∑ IMD</div>
                <h3>Iterative Modality Dialogue</h3>
                <p>
                    Three rounds of pairwise cross-attention (video‚Üîaudio, audio‚Üîtext, text‚Üîvideo)
                    allow each modality to query complementary cues from the others.
                    A learned stop gate halts fusion early when representations converge.
                    Final fused vector: 1536-D (3 √ó 512-D).
                </p>
            </div>

            <div class="component-card">
                <div class="card-tag">Prediction ¬∑ TSAG</div>
                <h3>Trait-Specific Attention Gate</h3>
                <p>
                    A separate MLP per trait computes a relevance mask over the 1536-D fused
                    representation, producing five trait-focused embeddings. A hierarchical
                    predictor first infers broad personality clusters, then regresses
                    each of the five Big Five scores jointly.
                </p>
            </div>

            <div class="component-card">
                <div class="card-tag">Training</div>
                <h3>Joint Loss &amp; Hierarchical Predictor</h3>
                <p>
                    Training minimises a weighted sum of cluster-level cross-entropy
                    (Œ± = 0.3) and trait-level MSE (1 ‚àí Œ± = 0.7).
                    The hierarchical design ensures the model captures both global personality
                    orientation and fine-grained per-trait variations simultaneously.
                </p>
            </div>

        </div>
    </div>
</div>

<!-- ===================== FACIAL HEATMAP ===================== -->
<div class="white_section_nerf w-container">
    <h2 class="grey-heading_nerf">Facial Dynamics ‚Äî DFGN Visualization</h2>
    <p class="paragraph-3 nerf_text" style="margin-bottom:10px;">
        The Dynamic Face Graph Network captures spatiotemporal variation in facial landmarks
        across different emotional expressions. The heatmaps below visualize DFGN activation
        magnitudes per landmark for representative expressions, showing which facial regions
        carry the most discriminative personality cues.
    </p>

    <img class="section-img" src="static/images/img1.png"
         alt="DFGN Facial Expression Heatmap"
         style="max-width:20%;"
         onerror="this.style.display='none'; this.nextElementSibling.style.display='block'">
    <div style="display:none; background:#f2f2f2; border-radius:8px; padding:60px 20px; text-align:center; margin:20px 25px 6px; color:#aaa; font-family:Roboto,sans-serif;">
        [ Heatmap image ‚Äî place <code>img1.png</code> in <code>static/images/</code> ]
    </div>
    <p class="fig-caption">
        Fig. 2. DFGN output activation magnitudes across different facial expressions.
        Warmer colors indicate higher activation, revealing landmark regions most predictive of personality traits.
    </p>
</div>

<!-- ===================== AUDIO VISUALIZATION ===================== -->
<div class="section nerf_section">
    <div class="w-container grey_container" style="padding-bottom:30px;">
        <h2 class="grey-heading_nerf">Audio Embedding ‚Äî PACN Visualization</h2>
        <p class="paragraph-3 nerf_text" style="margin-bottom:10px;">
            The PACN processes raw audio through dual-stream feature extraction.
            The visualization below compares the extracted waveform, high-level Wav2Vec2 embeddings (768-D),
            and the final capsule-output magnitudes (32 capsules) that form the 512-D audio embedding.
        </p>

        <img class="section-img" src="static/images/audio.png"
             alt="PACN Audio Embedding Visualization"
             style="max-width:70%;"
             onerror="this.style.display='none'; this.nextElementSibling.style.display='block'">
        <div style="display:none; background:#e8e8e8; border-radius:8px; padding:60px 20px; text-align:center; margin:20px 25px 6px; color:#aaa; font-family:Roboto,sans-serif;">
            [ Audio visualization ‚Äî place <code>audio.png</code> in <code>static/images/</code> ]
        </div>
        <p class="fig-caption">
            Fig. 3. (a) Extracted audio waveform. (b) High-level Wav2Vec2 features (W<sub>t</sub>).
            (c) Final audio embedding (F<sub>ADO</sub>): magnitudes of 32 output capsules from PACN.
        </p>
    </div>
</div>

<!-- ===================== RESULTS ===================== -->
<div class="white_section_nerf w-container">
    <h2 class="grey-heading_nerf">Results on ChaLearn LAP 2017</h2>
    <p class="paragraph-3 nerf_text" style="margin-bottom:18px;">
        TriadNet is evaluated on the official ChaLearn First Impressions V2 test split (~2,000 videos).
        Trait accuracy is defined as
        <em>Acc<sub>i</sub> = 1 ‚àí mean|ground-truth ‚àí predicted|</em>.
    </p>

    <div class="table-wrap">
        <table class="results-table">
            <thead>
                <tr>
                    <th>Trait</th>
                    <th>Accuracy (%)</th>
                    <th>MSE</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>Openness</td><td>97.12</td><td>0.0123</td></tr>
                <tr><td>Conscientiousness</td><td>96.45</td><td>0.0109</td></tr>
                <tr><td>Extraversion</td><td>96.83</td><td>0.0117</td></tr>
                <tr><td>Agreeableness</td><td>96.93</td><td>0.0124</td></tr>
                <tr><td>Neuroticism</td><td>96.95</td><td>0.0115</td></tr>
                <tr class="highlight-row">
                    <td>Mean Trait Performance</td><td>96.93</td><td>0.0116</td>
                </tr>
                <tr class="highlight-row">
                    <td>Cluster Accuracy</td><td>97.80</td><td>‚Äî</td>
                </tr>
            </tbody>
        </table>
    </div>
</div>

<!-- ===================== ABLATION ===================== -->
<div class="section nerf_section">
    <div class="w-container grey_container" style="padding-bottom:30px;">
        <h2 class="grey-heading_nerf">Ablation Study</h2>
        <p class="paragraph-3 nerf_text" style="margin-bottom:18px;">
            Each component is individually ablated to measure its contribution.
            Œî values are relative to the full model (Exp 0).
        </p>

        <div class="table-wrap">
            <table class="results-table">
                <thead>
                    <tr>
                        <th>#</th>
                        <th>Configuration</th>
                        <th>Cluster Acc</th>
                        <th>Avg Trait MSE</th>
                        <th>ŒîAcc</th>
                        <th>ŒîMSE</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="highlight-row">
                        <td>0</td><td>Full model (baseline)</td>
                        <td>0.9692</td><td>0.1051</td><td>‚Äî</td><td>‚Äî</td>
                    </tr>
                    <tr>
                        <td>1</td><td>Video only</td>
                        <td>0.8520</td><td>0.2102</td><td>‚àí0.1172</td><td>+0.1051</td>
                    </tr>
                    <tr>
                        <td>2</td><td>No IMD (naive concat)</td>
                        <td>0.9105</td><td>0.1498</td><td>‚àí0.0587</td><td>+0.0447</td>
                    </tr>
                    <tr>
                        <td>3</td><td>No TSAG</td>
                        <td>0.9223</td><td>0.1385</td><td>‚àí0.0469</td><td>+0.0334</td>
                    </tr>
                    <tr>
                        <td>4</td><td>Remove stop gate</td>
                        <td>0.9385</td><td>0.1237</td><td>‚àí0.0307</td><td>+0.0186</td>
                    </tr>
                    <tr>
                        <td>5</td><td>PACN ‚Üí Conv</td>
                        <td>0.9560</td><td>0.1152</td><td>‚àí0.0132</td><td>+0.0101</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <p class="paragraph-3 nerf_text" style="margin-top:8px; font-size:0.9rem;">
            Removing all audio and text (Exp 1) causes the largest drop, confirming the value of
            multimodal complementarity. The IMD cross-attention (Exp 2) and TSAG (Exp 3) both
            contribute substantially, while the stop gate (Exp 4) adds moderate robustness and
            replacing PACN with a standard convolution (Exp 5) reduces acoustic modeling quality.
        </p>
    </div>
</div>

<!-- ===================== BIBTEX ===================== -->
<div class="white_section_nerf grey_container w-container" style="padding-bottom:30px;">
    <h2 class="grey-heading_nerf">BibTeX</h2>
    <div class="bibtex">
        <pre><code>@article{pahari2025triadnet,
  author    = {Soham Pahari and Sandeep Chand Kumain and Lalit K. Awasthi and Maheep Singh},
  title     = {TriadNet: Graph-Enhanced Multimodal Learning for Personality Trait Assessment},
  journal   = {IEEE},
  year      = {2025},
  url       = {https://github.com/sohampahari/TriadNet}
}</code></pre>
    </div>
</div>

</body>

<footer style="padding-top:24px; padding-bottom:18px; text-align:center; font-family:Roboto,sans-serif; font-size:13px; color:#aaa; border-top:1px solid #eee;">
    This project page is inspired by
    <a href="https://github.com/nerfies/nerfies.github.io" style="color:#82abd0;">Nerfies</a>
    and
    <a href="https://sweetdreamer3d.github.io" style="color:#82abd0;">SweetDreamer</a>.
    &copy; 2025 TriadNet Authors. All rights reserved.
</footer>

</html>
